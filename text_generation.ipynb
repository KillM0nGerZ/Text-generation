{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text-generation.ipynb",
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOUm0uodbzPN"
      },
      "outputs": [],
      "source": [
        "!pip install pythainlp==3.0.5\n",
        "!curl 'https://raw.githubusercontent.com/kokkoks/thai-joke-sentence-generator/main/siaw_caption.txt' > siaw_caption.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets transformers==4.11.2\n",
        "!pip install sentencepiece"
      ],
      "metadata": {
        "id": "K21LoS6Cb5s9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from pythainlp import word_tokenize\n",
        "# datasets = load_dataset(\"text\", data_files={\"train\": path_to_train.txt, \"validation\": path_to_validation.txt}"
      ],
      "metadata": {
        "id": "Feac4e4IuBx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#เตรียมข้อมูล"
      ],
      "metadata": {
        "id": "5ZSrhcCfJoF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def txt_to_lst(file_path):\n",
        "  stopword=open(file_path,\"r\")\n",
        "  lines = stopword.read().split('\\n')\n",
        "  return lines"
      ],
      "metadata": {
        "id": "tsxRa5ucvdVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def export2txt(name,item):\n",
        "  with open(name, 'w') as f:\n",
        "    for i in item:\n",
        "      f.write(' '.join(word_tokenize(i)))\n",
        "      f.write('\\n')"
      ],
      "metadata": {
        "id": "wmgGfn4d7gCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = txt_to_lst('/content/siaw_caption.txt')\n",
        "count = len(data)\n",
        "result_percent = count*0.2\n",
        "result_split = int(round(count - result_percent))\n",
        "train = data[:result_split]\n",
        "valid = data[result_split:]"
      ],
      "metadata": {
        "id": "RXr_kBhD6JxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "export2txt('train.txt',train)\n",
        "export2txt('valid.txt',valid)"
      ],
      "metadata": {
        "id": "IXFw1fmz8h8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = load_dataset(\"text\", data_files={\"train\": \"/content/train.txt\", \n",
        "                                            \"validation\":\"/content/train.txt\"})"
      ],
      "metadata": {
        "id": "LNDPaCKW9Hwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets[\"train\"][10]"
      ],
      "metadata": {
        "id": "OAUR_5Se927Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import ClassLabel\n",
        "import random\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "def show_random_elements(dataset, num_examples=10):\n",
        "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
        "    picks = []\n",
        "    for _ in range(num_examples):\n",
        "        pick = random.randint(0, len(dataset)-1)\n",
        "        while pick in picks:\n",
        "            pick = random.randint(0, len(dataset)-1)\n",
        "        picks.append(pick)\n",
        "    \n",
        "    df = pd.DataFrame(dataset[picks])\n",
        "    for column, typ in dataset.features.items():\n",
        "        if isinstance(typ, ClassLabel):\n",
        "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
        "    display(HTML(df.to_html()))"
      ],
      "metadata": {
        "id": "ZGQXEcbz-HF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_random_elements(datasets[\"train\"])"
      ],
      "metadata": {
        "id": "b8yLVc5t-L_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint = \"airesearch/wangchanberta-base-att-spm-uncased\"\n",
        "# model_checkpoint = \"gpt2\"\n",
        "# model_checkpoint = \"google/mt5-base\""
      ],
      "metadata": {
        "id": "PJ8VUMNP-N4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "    \n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
      ],
      "metadata": {
        "id": "_6gjYia4-UiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"])"
      ],
      "metadata": {
        "id": "FwotJwde-dSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets = datasets.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\"text\"])"
      ],
      "metadata": {
        "id": "qHdqdRaU-jrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets[\"train\"][1]"
      ],
      "metadata": {
        "id": "yP1dH97K-lPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 128"
      ],
      "metadata": {
        "id": "-sEhgggf-nVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def group_texts(examples):\n",
        "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
        "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
        "    total_length = (total_length // block_size) * block_size\n",
        "    result = {\n",
        "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
        "        for k, t in concatenated_examples.items()\n",
        "    }\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result"
      ],
      "metadata": {
        "id": "7jFM8m0v-qkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lm_datasets = tokenized_datasets.map(\n",
        "    group_texts,\n",
        "    batched=True,\n",
        "    batch_size=1000,\n",
        "    num_proc=4,\n",
        ")"
      ],
      "metadata": {
        "id": "P8HSj70E-sWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(lm_datasets[\"train\"][1][\"input_ids\"])"
      ],
      "metadata": {
        "id": "xa8LMbAM-uZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#เตรียม Fine tune Model"
      ],
      "metadata": {
        "id": "Z9aPqRjsJtL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM,AutoModelForSeq2SeqLM,AutoModel\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"airesearch/wangchanberta-base-att-spm-uncased\")\n",
        "# model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "# model = AutoModelForSeq2SeqLM.from_pretrained(\"google/mt5-base\")"
      ],
      "metadata": {
        "id": "h_IAic6n-woD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments"
      ],
      "metadata": {
        "id": "uHV8gmCG-1q8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "training_args = TrainingArguments(\n",
        "    f\"{model_name}-finetuned\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    num_train_epochs=200,\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    per_device_train_batch_size=1,  \n",
        "    per_device_eval_batch_size=1\n",
        ")"
      ],
      "metadata": {
        "id": "KKufXtd--4rm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=lm_datasets[\"train\"],\n",
        "    eval_dataset=lm_datasets[\"validation\"],\n",
        ")"
      ],
      "metadata": {
        "id": "iKfH_61A-6YI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "mhIbs8cz-8zy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model('model-wangchanberta')\n",
        "# trainer.save_model('model-gpt2')\n",
        "# trainer.save_model('model-mT5')"
      ],
      "metadata": {
        "id": "NpjifCnkEtYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ทดสอบ Model"
      ],
      "metadata": {
        "id": "lzmmMZwiJeGt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"airesearch/wangchanberta-base-att-spm-uncased\")\n",
        "model = AutoModelForCausalLM.from_pretrained('model-wangchanberta')\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "# model = AutoModelForCausalLM.from_pretrained('model-gpt2')\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"google/mt5-base\")\n",
        "# model = AutoModelForSeq2SeqLM.from_pretrained('/model/model-mT5')"
      ],
      "metadata": {
        "id": "u-3oClZX--dS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cpu')"
      ],
      "metadata": {
        "id": "okCQ5K0SC26X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"ความรักก็เหมือน\""
      ],
      "metadata": {
        "id": "RHPe5oA6C6bt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_text = text.strip().replace(\" \",\"\")\n",
        "prepared_Text = preprocess_text\n",
        "print(\"original text preprocessed: \\n\", preprocess_text)\n",
        "tokenized_text = tokenizer.encode(prepared_Text,return_tensors=\"pt\").to(device)"
      ],
      "metadata": {
        "id": "JFcH9pvsCviC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_generate =  model.generate(tokenized_text,\n",
        "                                    min_length=30,\n",
        "                                    max_length=100,\n",
        "                                    early_stopping=True)\n",
        "output = tokenizer.decode(text_generate[0], skip_special_tokens=True,clean_up_tokenization_spaces=True)\n",
        "print(\"original text: \\n\"+preprocess_text)\n",
        "print(\"=\"*100)\n",
        "print(\"generate text: \\n\"+output.strip().capitalize())"
      ],
      "metadata": {
        "id": "rJe0IzoMBwtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Result"
      ],
      "metadata": {
        "id": "QWJrd6jg3DT3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WangChanBERTa\n",
        "ความรักก็เหมือนชม ชมา   1  1 1 1 1 1 1 1 1 1 11 111111 111111111111111111111111111111111111111111111111111111111\n",
        "\n"
      ],
      "metadata": {
        "id": "Vl67O4e72vgL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT-2\n",
        "ความรักก็เหมือน หาม จั่ว   แต่ ถ้า เธอ มี ๆ   เรา จะ เจอ �\n",
        "\n"
      ],
      "metadata": {
        "id": "WUduHFr_24nB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## google/mT5-base\n",
        "ความรักก็เหมือน... ความรักก็เหมือน... ความรักก็เหมือน... ความรักก็เหมือน... ความรักก็เหมือน... ความรักก็เหมือน"
      ],
      "metadata": {
        "id": "f87rF-w62730"
      }
    }
  ]
}